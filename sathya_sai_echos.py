# -*- coding: utf-8 -*-
"""sathya_sai_echos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U7jQGmMRv0n2OK8Giy3lNJ9VSrsFrTp3
"""

import streamlit as st
import replicate
import os

# App title
st.set_page_config(page_title = "ğŸ¦™ğŸ’¬ Llama 2 Chatbot")


# Replicate Credentials
with st.sidebar:
  st.title('ğŸ¦™ğŸ’¬ Llama 2 Chatbot')
  if "REPLICATE_API_TOKEN" in st.secrets:
    st.success("API key already provided", icon = "âœ…")
    replicate_api = st.secrets["REPLICATE_API_TOKEN"]
  else:
    replicate_api = st.text_input("Enter Replicate API token:", type = "password")
    if not (replicate_api.startswith("r8_") and len(replicate_api == 40)):
      st.warning("Please enter your credentials!", icon = "âš ï¸")
    else:
      st.success("Proceed to entering your prompt message!", icon = "'ğŸ‘‰")
os.environ["REPLOCATE_API_TOKEN"] = replicate_api


# Store LLM Generated responses
if "messages" not in st.session_state.keys():
  st.session_state.messages = [{"role":"assistant", "content": "Hello Sai Bangaru!! What brings you here today ? ğŸ˜Š"}]


# Display or clear chat messages
for message in st.session_state.messages:
  with st.chat_message(message["role"]):
    st.write(message["content"])


def clear_chat_history():
  st.session_state.messages = [{"role": "assistant", "content": "Are there any other things that you want me to help you with? ğŸ˜Š"}]
st.sidebar.button("Clear Chat history", on_click = clear_chat_history)

# Function for generating LLaMA2 response
def generate_llama2_response(prompt_input):
  string_dialogue = "You are a helpful assistant. You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'"
  for dict_message in st.session_state.messages:
    if dict_message["role"] == "user":
      string_dialogue += "User: " + dict_message["content"] + "\\n\\n"
    else:
      string_dialogue += "Assistant: " +dict_message["content"] +"\\n\\n"
  output = replicate.run("replicate/llama-2-70b-chat:58d078176e02c219e11eb4da5a02a7830a283b14cf8f94537af893ccff5ee781",
                         input = {"prompt": f"{string_dialogue} {prompt_input} Assistant: ",
                                  "temperature":0.1, "top_p":0.9, "max_length":512, "repetition_penalty":1})
  return output

# User-provided prompt
if prompt := st.chat_input(disabled = not replicate_api):
  st.session_state.messages.append({"role":"user","content":prompt})
  with st.chat_message("user"):
    st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
  with st.chat_message("assistant"):
    with st.spinner("Thinking..."):
      response = generate_llama2_response(prompt)
      placeholder = st.empty()
      full_response = ""
      for item in response:
        full_response += item
        placeholder.markdown(full_response)
      placeholder.markdown(full_response)
  message = {"role": "assistant", "content": full_response}
  st.session_state.message.append(message)

